

# ✅ 전체 프로젝트 플랜 정리: RL 기반 UI 최적화 (WaveUI 단일 데이터셋 활용)

-----

## 🎯 목표

> **WaveUI-25K 데이터셋을 기반으로 UI 레이아웃을 최적화하는 강화학습 시스템을 구축합니다.** WaveUI 데이터에서 추출한 특징과 모의 사용자 상호작용을 통해 CTR (클릭률) 및 체류 시간(dwell time) 등의 사용자 반응을 시뮬레이션하고, 이를 **reward**로 정의하여 UI 레이아웃을 조정하는 **agent**가 높은 보상을 유도하도록 훈련합니다.

-----

## 🔶 Phase 1. WaveUI 데이터 분석 및 특성 추출

### 1-1. WaveUI-25K 데이터셋 확보 및 분석

| 데이터셋           | 용도                                        | 링크                                                                |
| -------------- | ----------------------------------------- | ----------------------------------------------------------------- |
| **WaveUI-25K** | 웹 기반 UI 구조 25K개 이상 (버튼, 텍스트 등 레이아웃 요소 포함) | [WaveUI (HF)](https://huggingface.co/datasets/Voxel51/WaveUI-25k) |

📌 **목표**: WaveUI 데이터셋의 구조(JSON 또는 관련 형식)를 면밀히 분석하여 UI 요소(버튼, 텍스트 필드, 이미지 등)의 종류, 크기, 위치, 계층 구조 등을 파악합니다. 이를 통해 UI 레이아웃을 설명하는 데 필요한 **핵심적인 특성(feature)들을 정의**합니다.

-----

### 1-2. UI Layout 특성 추출 및 벡터화 (State 구성)

WaveUI 데이터에서 다음 방식들을 고려하여 UI 레이아웃을 상태(state) 벡터로 표현합니다.

  * **방식 A: Grid + Channel 방식 (20x20xC Tensor)**
      * UI를 특정 크기의 그리드(예: 20x20)로 나누고, 각 셀에 존재하는 UI 요소의 종류(버튼, 텍스트, 이미지 등)를 원-핫 인코딩하여 채널(C)로 표현합니다. 이는 이미지와 유사한 형태로 UI를 표현하여 CNN 계열 모델에 적합하게 만듭니다.
  * **방식 B: 통계 기반 Fixed Vector (추천)**
      * UI 내 특정 요소의 개수, 평균/분포 위치, 특정 중요 요소(CTA 버튼 등)의 크기 및 화면 비율, 텍스트 밀도, UI 복잡도(요소 개수) 등 **UI의 고수준 통계적 특성**을 추출하여 고정된 길이의 벡터로 표현합니다. 이 방식은 초기 구현 속도와 유연성 측면에서 유리합니다.
  * **방식 C: GNN용 Graph 구조 (선택적)**
      * UI 요소를 노드(Node)로, 요소 간의 공간적/계층적 관계를 엣지(Edge)로 정의하여 그래프 형태로 표현합니다. 복잡한 UI 구조를 반영하는 데 유리하지만, 구현 난이도가 높습니다.

> 🔧 **초기에는 방식 B(통계 기반 Fixed Vector)를 추천**합니다. WaveUI 데이터의 특성을 고려하여, CTR 및 체류 시간에 영향을 미칠 것으로 예상되는 UI 특성들을 중점적으로 정의합니다.

-----

## 🔶 Phase 2. 모의 사용자 반응 시뮬레이터 학습 (Reward Simulator)

> **WaveUI 데이터셋의 UI 레이아웃 특성을 입력으로 받아, 예상 CTR 및 체류 시간(dwell time)을 출력하는 시뮬레이터 모델을 구축합니다.** 이 모델은 강화학습에서 **보상 함수로 사용**됩니다. 실제 사용자 로그가 없으므로, WaveUI 데이터의 UI 특성(예: CTA 버튼의 크기/위치, 중요 텍스트의 가독성, 요소 간 여백 등)과 UI/UX 디자인 원칙을 기반으로 가상의 사용자 반응 라벨을 생성하거나, 간단한 휴리스틱 모델을 초기 시뮬레이터로 사용합니다.

### 2-1. 가상 사용자 반응 라벨 구성 (CTR & Dwell Time)

  * **CTR 라벨**: WaveUI 내 특정 '클릭 가능한' 요소(버튼, 링크 등)의 특성(크기, 색상 대비, 위치, 주변 요소와의 관계 등)을 분석하여 해당 요소의 \*\*모의 클릭률(simulated CTR)\*\*을 추정하는 라벨을 구성합니다. 예를 들어, "화면 중앙에 위치한 크고 대비가 높은 버튼"은 높은 CTR을 가질 것이라고 가정할 수 있습니다.
  * **체류 시간(Dwell Time) 라벨**: UI의 정보 밀도, 가독성, 시각적 복잡도 등을 기반으로 \*\*모의 체류 시간(simulated dwell time)\*\*을 추정하는 라벨을 구성합니다. 예를 들어, "잘 정돈되고 정보가 명확한 UI"는 높은 체류 시간을 가질 것이라고 가정할 수 있습니다.
  * **전문가 규칙 기반 (초기 방식)**: 초기에는 UI/UX 전문가의 지식이나 일반적인 디자인 원칙(예: 피츠의 법칙, 게슈탈트 원리 등)을 기반으로 UI 특성-사용자 반응 간의 관계를 규칙으로 정의하여 가상의 라벨을 생성합니다.

### 2-2. Reward Simulator 모델 학습

  * **입력**: WaveUI에서 추출한 UI 레이아웃 벡터 (Phase 1-2에서 정의한 state 벡터)
  * **출력**: 모의 CTR ($\\in [0, 1]$) 또는 모의 체류 시간 (양의 실수)
  * **모델 후보**:
      * `XGBoost`, `LightGBM`, `GradientBoostingClassifier/Regressor`
      * `MLPClassifier/Regressor` (TensorFlow/Keras)
      * `LogisticRegression/LinearRegression` (baseline)
  * **학습 목표**: UI 레이아웃 특성과 생성된 가상 사용자 반응 라벨 간의 관계를 학습하여, 새로운 UI 레이아웃이 주어졌을 때 CTR 및 체류 시간을 예측합니다.

-----

## 🔶 Phase 3. 강화학습 환경 구성 (OpenAI Gym 스타일)

> **WaveUI 데이터 기반으로 정의된 UI 상태, UI 조작 액션, 그리고 Reward Simulator에서 얻은 보상을 바탕으로 OpenAI Gym 스타일의 강화학습 환경 클래스를 정의합니다.**

### 환경 요소 정의

| 구성요소           | 설명                                                         |
| -------------- | ---------------------------------------------------------- |
| **state** | 현재 UI 벡터 (Phase 1-2에서 정의한 WaveUI 기반 벡터)              |
| **action** | UI 요소 조정. WaveUI 데이터 구조를 기반으로 **구체적인 액션 정의**가 필요합니다. 예: \<br\> - 특정 UI 요소 (버튼, 텍스트 블록 등)의 크기/위치 변경 \<br\> - 요소의 색상/폰트 변경 \<br\> - 요소 추가/삭제 (초기에는 변경/이동에 집중) \<br\> - 요소 간 간격 조정 등                                |
| **reward** | **Reward Simulator (Phase 2)가 예측한 CTR 값 또는 체류 시간, 혹은 이 둘의 조합** |
| **transition** | agent의 action을 반영하여 새로운 UI 상태 생성. UI 조작 규칙과 WaveUI의 구조적 제약을 고려하여 유효한 UI 상태가 되도록 구현 |

-----

## 🔶 Phase 4. RL 에이전트 학습

> **구축된 강화학습 환경에서 에이전트를 훈련하여 UI 조정 정책을 학습하고 CTR 및 체류 시간을 최대화합니다.**

### 알고리즘 후보

  * **DQN (Deep Q-Network) ✅ 추천**: 이산적인 UI 조정 액션을 정의하기에 적합하며, 비교적 안정적인 학습 성능을 기대할 수 있습니다.
  * PPO / A2C (선택적): 액션 공간이 연속적이거나 정책 기반의 학습이 필요한 경우 고려할 수 있습니다.
  * Policy Gradient (baseline): 기본적인 정책 학습 알고리즘으로 초기 실험에 활용할 수 있습니다.

### 학습 목표

  * 에이전트는 주어진 UI 상태에서 최적의 UI 조정 액션을 선택하는 정책을 학습합니다.
  * 에피소드 종료 시점 또는 특정 단계를 거쳤을 때 누적된 보상(CTR, 체류 시간)을 최대화하는 것을 목표로 합니다.

-----

## 🔶 Phase 5. 평가 및 시각화

| 평가 지표             | 설명                                                         |
| ----------------- | ---------------------------------------------------------- |
| 평균 CTR / 체류 시간  | 테스트 UI 데이터셋에서 최적화된 UI 레이아웃에 대한 Reward Simulator의 예측값 |
| A/B 테스트 시뮬레이션   | 에이전트가 최적화한 UI와 초기/기존 UI 간의 Reward Simulator 예측값 비교 |
| 행동 Trajectory 시각화 | 에이전트가 초기 UI에서 최종 최적화된 UI에 도달하기까지의 UI 변화 과정(액션 시퀀스)을 로그 또는 GIF 형태로 시각화하여 에이전트의 학습 메커니즘 분석 |

-----

# ✅ 요약된 전체 흐름도 (WaveUI 단일 데이터셋 기반)

```
WaveUI Dataset ──▶ UI 특성 추출 및 벡터화 (State)
         │
         └─────────▶ 가상 사용자 반응 라벨 구성 (CTR & Dwell Time)
                              │
                              ▼
                        CTR/Dwell Time 예측 모델 (Reward Simulator)
                              │
        +─────────────────────┘
        │
      [Gym 환경 구성]  ◀─────── UI 변경 action
        │
     RL Agent 학습 (DQN)
        │
   최적 UI 레이아웃 도출
```

