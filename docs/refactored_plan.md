
# ✅ 전체 프로젝트 플랜 리팩토링: 원시 데이터 중심 파이프라인

-----

## 🎯 핵심 목표

> **UI 레이아웃 최적화를 위한 강화학습 시스템을 구축하되, 모든 파이프라인(데이터, 보상 모델, RL 환경, 시각화)이 단일 원시 데이터 소스를 바라보게 하여 데이터 불일치 문제를 원천적으로 해결하고, 각 모듈의 유연성과 확장성을 극대화한다.**

-----

## 🔶 문제 정의

기존 파이프라인은 두 가지 심각한 문제가 존재했다.

1.  **시각화 불가**: `labeled_waveui.csv`는 통계적으로 집계된 데이터이므로, 개별 UI 요소 정보가 유실되어 시각적 복원이 불가능했다.
2.  **강화학습 불가**: 집계된 데이터로는 "버튼 A를 옮긴다"와 같은 개별 요소 조작을 학습할 수 없었다.

이 문제를 해결하기 위해, 모든 모듈이 전처리되지 않은 원시 데이터에 직접 접근하여 각자의 목적에 맞는 특징을 동적으로 추출하는 방식으로 파이프라인을 재설계한다.

-----

## 🔶 Phase 1. 원시 데이터 파이프라인 구축

### 1-1. 원시 데이터 생성 (`create_raw_element_data.py`)

*   **목표**: Hugging Face 데이터셋을 다운로드하여, 각 UI 요소의 정보를 `screenshot_id`와 함께 거의 그대로 저장하는 단일 CSV 파일을 생성한다.
*   **스크립트**: `create_raw_element_data.py`
*   **결과물**: `data/raw_elements.csv` (그룹화나 집계 없는 순수 요소 데이터)

### 1-2. Reward Simulator 재설계 (동적 특징 추출)

*   **목표**: `reward_simulator_trainer.py`가 `raw_elements.csv`를 직접 읽어, 학습에 필요한 통계적 특징을 **동적으로 계산**하도록 수정한다.
*   **수정 대상**: `reward_simulator_trainer.py`
*   **로직**: `raw_elements.csv` 로드 → `screenshot_id`로 그룹화 → 학습 직전에 통계 특징 추출 함수 실행 → 모델 재학습

-----

## 🔶 Phase 2. 강화학습 환경 재설계

### 2-1. `rl_env.py` 재설계

*   **목표**: `PagePilotEnv`가 `screenshot_id`만으로 상태를 관리하고, 개별 요소 정보에 직접 접근할 수 있도록 수정한다.
*   **수정 대상**: `rl_env.py`
*   **핵심 변경 사항**:
    *   `__init__`: `raw_elements.csv`를 `self.raw_data`로 로드한다.
    *   **State 표현 변경**: RL 환경의 `state`는 이제 숫자 벡터가 아닌, **UI 요소 딕셔너리의 리스트**가 된다. (e.g., `[{'type': 'button', 'bbox': [...]}, ...]`).
    *   `reset()`: `screenshot_id`를 샘플링하고, `self.raw_data`에서 해당 요소 리스트를 찾아와 초기 `state`로 설정한다.
    *   `step(action)`: `state` (요소 리스트)를 직접 수정하여 다음 `state`를 생성한다.
    *   `_get_reward()`: 현재 `state` (요소 리스트)를 받아 동적으로 통계 특징을 계산하고, 이를 Reward Simulator에 전달하여 보상을 받는다.

### 2-2. DQN 입력 벡터 변환기 추가

*   **목표**: `rl_env.py` 내에, `state` (요소 리스트)를 신경망에 입력할 수 있는 고정 길이의 숫자 벡터로 변환하는 헬퍼 함수를 추가한다.
*   **함수명**: `_state_to_vector(state)`
*   **역할**: DQN 에이전트가 학습을 위해 `step`이나 `reset`의 반환값을 사용할 때, 이 함수를 거쳐 최종 입력 벡터를 얻는다.

-----

## 🔶 Phase 3. 에이전트 학습 및 시각화

### 3-1. `dqn_trainer.py` 수정

*   **목표**: 새로운 환경(`PagePilotEnv`)과 상호작용하도록 훈련 루프를 수정한다.
*   **수정 대상**: `dqn_trainer.py`
*   **로직**: 환경으로부터 `state` (요소 리스트)를 받은 후, `_state_to_vector()`를 호출하여 신경망 입력용 텐서로 변환하는 코드를 추가한다.

### 3-2. `evaluation.py` 시각화 함수 재작성

*   **목표**: `evaluation.py`의 시각화 함수가 새로운 `state` (요소 리스트)를 직접 받아 정확한 UI를 렌더링하도록 재작성한다.
*   **수정 대상**: `evaluation.py`의 `render_ui_from_vector` 함수
*   **변경**: `render_ui_from_elements(elements_list, file_path)` 와 같은 형태로 변경. 이제 이 함수는 복잡한 벡터 해석 없이, 요소 리스트를 순회하며 사각형만 그리면 되므로 훨씬 간단하고 정확해진다.

-----

## ✅ 요약된 전체 흐름도 (리팩토링 후)

```
                               +-------------------------+
                               |  raw_elements.csv       |
                               | (Single Source of Truth)|
                               +-----------+-------------+
                                           |
           +-------------------------------+-------------------------------+
           |
           ▼
+----------+-------------+     +----------------------+     +-------------------------+
| Reward Simulator       |     | RL Environment (Env) |     | Visualization           |
| (Dynamic Features)     |     | (State as List)      |     | (Direct Rendering)      |
+------------------------+     +----------+-----------+     +-------------------------+
                                          |
                                          ▼
                               +----------+-----------+
                               | DQN Agent            |
                               | (Vectorized Input)   |
                               +----------------------+

```
